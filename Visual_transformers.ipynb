{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data/cifar-10\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data/cifar-10\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data/cifar-10', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='./data/cifar-10', train=False, download=True, transform=transform)\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), 50000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape, len(train_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 32, 32), 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].numpy().shape, train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 32, 32), 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].numpy().shape, train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 4\n",
    "for i in range(0, 32, patch_size):\n",
    "    for j in range(0, 32, patch_size):\n",
    "        patch = train_data[0][0][:, i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "        # do something with patch...\n",
    "\n",
    "print(patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "           [ 4.,  5.,  6.,  7.],\n",
       "           [ 8.,  9., 10., 11.],\n",
       "           [12., 13., 14., 15.]],\n",
       " \n",
       "          [[16., 17., 18., 19.],\n",
       "           [20., 21., 22., 23.],\n",
       "           [24., 25., 26., 27.],\n",
       "           [28., 29., 30., 31.]],\n",
       " \n",
       "          [[32., 33., 34., 35.],\n",
       "           [36., 37., 38., 39.],\n",
       "           [40., 41., 42., 43.],\n",
       "           [44., 45., 46., 47.]]]]),\n",
       " tensor([[[[[[ 0.,  1.],\n",
       "             [ 4.,  5.]],\n",
       " \n",
       "            [[ 2.,  3.],\n",
       "             [ 6.,  7.]]],\n",
       " \n",
       " \n",
       "           [[[ 8.,  9.],\n",
       "             [12., 13.]],\n",
       " \n",
       "            [[10., 11.],\n",
       "             [14., 15.]]]],\n",
       " \n",
       " \n",
       " \n",
       "          [[[[16., 17.],\n",
       "             [20., 21.]],\n",
       " \n",
       "            [[18., 19.],\n",
       "             [22., 23.]]],\n",
       " \n",
       " \n",
       "           [[[24., 25.],\n",
       "             [28., 29.]],\n",
       " \n",
       "            [[26., 27.],\n",
       "             [30., 31.]]]],\n",
       " \n",
       " \n",
       " \n",
       "          [[[[32., 33.],\n",
       "             [36., 37.]],\n",
       " \n",
       "            [[34., 35.],\n",
       "             [38., 39.]]],\n",
       " \n",
       " \n",
       "           [[[40., 41.],\n",
       "             [44., 45.]],\n",
       " \n",
       "            [[42., 43.],\n",
       "             [46., 47.]]]]]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.arange(0.,48).reshape(1, 3, 4, 4) \n",
    "image, image.unfold(2, 2, 2).unfold(3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  4.,  5.],\n",
       "          [ 2.,  3.,  6.,  7.],\n",
       "          [ 8.,  9., 12., 13.],\n",
       "          [10., 11., 14., 15.]],\n",
       "\n",
       "         [[16., 17., 20., 21.],\n",
       "          [18., 19., 22., 23.],\n",
       "          [24., 25., 28., 29.],\n",
       "          [26., 27., 30., 31.]],\n",
       "\n",
       "         [[32., 33., 36., 37.],\n",
       "          [34., 35., 38., 39.],\n",
       "          [40., 41., 44., 45.],\n",
       "          [42., 43., 46., 47.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unfold(2,2,2).unfold(3,2,2).reshape(1, -1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  4.,  5., 16., 17., 20., 21., 32., 33., 36., 37.],\n",
       "         [ 2.,  3.,  6.,  7., 18., 19., 22., 23., 34., 35., 38., 39.],\n",
       "         [ 8.,  9., 12., 13., 24., 25., 28., 29., 40., 41., 44., 45.],\n",
       "         [10., 11., 14., 15., 26., 27., 30., 31., 42., 43., 46., 47.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unfold(2,2,2).unfold(3,2,2).reshape(1, -1, 4, 4).permute(0, 2, 1, 3) # B x N x C x P\n",
    "image.unfold(2,2,2).unfold(3,2,2).reshape(1, -1, 4, 4).permute(0, 2, 1, 3).reshape(1, 4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size = 32, patch_size = 4, in_chans = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size   = img_size\n",
    "        self.patch_size = patch_size    # P\n",
    "        self.in_chans   = in_chans      # C\n",
    "        self.embed_dim  = embed_dim     # D\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2        # N = H*W/P^2\n",
    "        self.flatten_dim = patch_size * patch_size * in_chans   # P^2*C\n",
    "        \n",
    "        self.proj = nn.Linear(self.flatten_dim, embed_dim) # (P^2*C,D)\n",
    "\n",
    "        self.position_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embed_dim))\n",
    "        self.class_embed    = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.reshape(1, -1, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(B, self.num_patches, -1)\n",
    "\n",
    "        x = self.proj(x)\n",
    "\n",
    "        cls_emb = self.class_embed.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_emb, x), dim = 1)\n",
    "\n",
    "        x = x + self.position_embed\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2749, -0.2905, -0.5764,  ...,  0.1573,  0.1988,  0.0659],\n",
       "          [ 0.2592, -0.1101, -0.4806,  ...,  0.1212,  0.2012, -0.0917],\n",
       "          ...,\n",
       "          [ 0.2375, -0.1225, -0.2667,  ...,  0.0106,  0.0968, -0.0470],\n",
       "          [ 0.1667, -0.2141, -0.4615,  ...,  0.0946,  0.1268, -0.0650],\n",
       "          [ 0.1417, -0.0721, -0.2739,  ...,  0.0598,  0.1200, -0.0240]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2669, -0.4007, -0.8260,  ...,  0.3725,  0.2589, -0.0055],\n",
       "          [ 0.1941, -0.2255, -0.5947,  ...,  0.2060,  0.1223, -0.0429],\n",
       "          ...,\n",
       "          [ 0.4147, -0.3022, -0.4285,  ...,  0.2373,  0.2337,  0.0757],\n",
       "          [ 0.1847, -0.1846, -0.6169,  ...,  0.2372,  0.3218,  0.0298],\n",
       "          [ 0.2384, -0.3314, -0.4986,  ...,  0.0724,  0.1768,  0.0325]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2773, -0.3076, -0.6477,  ...,  0.1365,  0.2107, -0.0329],\n",
       "          [ 0.2051, -0.3183, -0.6349,  ...,  0.2350,  0.1877,  0.0254],\n",
       "          ...,\n",
       "          [ 0.4525, -0.3900, -0.6842,  ...,  0.1860,  0.2813, -0.1214],\n",
       "          [ 0.2067, -0.2506, -0.7215,  ...,  0.0602,  0.2362, -0.1638],\n",
       "          [ 0.2302, -0.2078, -0.3424,  ...,  0.2011,  0.3096,  0.0850]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2406, -0.2938, -0.6553,  ...,  0.1669,  0.2209,  0.0109],\n",
       "          [ 0.1794, -0.2133, -0.5267,  ...,  0.2323,  0.1725, -0.0658],\n",
       "          ...,\n",
       "          [ 0.2462, -0.2880, -0.7461,  ...,  0.0377,  0.3148, -0.1522],\n",
       "          [ 0.2854, -0.0549, -0.4873,  ...,  0.1373,  0.1004, -0.0754],\n",
       "          [ 0.1342, -0.1310, -0.6161,  ...,  0.2428,  0.3785,  0.1147]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.4791, -0.4969, -1.1706,  ...,  0.2298,  0.3213, -0.0238],\n",
       "          [ 0.2770, -0.3085, -1.0546,  ...,  0.3327,  0.0440,  0.0734],\n",
       "          ...,\n",
       "          [ 0.1693, -0.2043, -0.2704,  ...,  0.0574,  0.1503,  0.0505],\n",
       "          [ 0.1006,  0.0051, -0.1219,  ...,  0.1207,  0.0855, -0.0672],\n",
       "          [ 0.1062, -0.1047, -0.1512,  ...,  0.0320,  0.0901, -0.0166]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1310, -0.3336, -0.6169,  ...,  0.1608,  0.2343,  0.0418],\n",
       "          [ 0.1704, -0.2742, -0.6504,  ...,  0.2216,  0.1668,  0.0345],\n",
       "          ...,\n",
       "          [ 0.0903, -0.0844, -0.0708,  ...,  0.0460,  0.0384, -0.0150],\n",
       "          [ 0.0688, -0.0706, -0.1383,  ...,  0.0833,  0.1101, -0.0380],\n",
       "          [ 0.1575, -0.1104, -0.2014,  ...,  0.0273,  0.0778, -0.0129]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([10, 65, 768]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed = PatchEmbedding()\n",
    "\n",
    "embeddings = patch_embed(torch.stack([train_data[i][0] for i in range(10)]))\n",
    "\n",
    "embeddings, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, num_heads = 4, bias = False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.num_heads   = num_heads\n",
    "        self.head_dim    = embed_dim // num_heads\n",
    "\n",
    "        self.query   = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.key     = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.value   = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.out     = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        q = self.query(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.key(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.value(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # do NOT use causal attention as we are not dealing with sequential data (image patches are unordered)\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v).permute(0, 2, 1, 3).reshape(B, N, self.embed_dim)\n",
    "\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4521,  0.3022, -0.4003,  ...,  0.1410, -0.1591,  0.1097],\n",
       "         [ 0.4379,  0.2964, -0.3942,  ...,  0.1399, -0.1582,  0.1012],\n",
       "         [ 0.4376,  0.2990, -0.3890,  ...,  0.1354, -0.1567,  0.0946],\n",
       "         ...,\n",
       "         [ 0.4447,  0.2999, -0.3981,  ...,  0.1387, -0.1594,  0.1015],\n",
       "         [ 0.4448,  0.3031, -0.3942,  ...,  0.1397, -0.1557,  0.1006],\n",
       "         [ 0.4457,  0.2967, -0.4000,  ...,  0.1437, -0.1589,  0.1051]],\n",
       "\n",
       "        [[ 0.4397,  0.3206, -0.3868,  ...,  0.1627, -0.1734,  0.0518],\n",
       "         [ 0.4380,  0.3194, -0.3835,  ...,  0.1600, -0.1794,  0.0408],\n",
       "         [ 0.4349,  0.3249, -0.3820,  ...,  0.1565, -0.1751,  0.0447],\n",
       "         ...,\n",
       "         [ 0.4388,  0.3122, -0.3854,  ...,  0.1745, -0.1755,  0.0433],\n",
       "         [ 0.4333,  0.3087, -0.3845,  ...,  0.1708, -0.1781,  0.0381],\n",
       "         [ 0.4326,  0.3140, -0.3814,  ...,  0.1620, -0.1773,  0.0423]],\n",
       "\n",
       "        [[ 0.4510,  0.3417, -0.4013,  ...,  0.1494, -0.1787,  0.1014],\n",
       "         [ 0.4403,  0.3454, -0.3978,  ...,  0.1387, -0.1798,  0.0902],\n",
       "         [ 0.4475,  0.3547, -0.4007,  ...,  0.1342, -0.1803,  0.0904],\n",
       "         ...,\n",
       "         [ 0.4429,  0.3429, -0.3995,  ...,  0.1406, -0.1882,  0.0903],\n",
       "         [ 0.4431,  0.3489, -0.4042,  ...,  0.1345, -0.1839,  0.0907],\n",
       "         [ 0.4404,  0.3261, -0.3995,  ...,  0.1582, -0.1815,  0.0947]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4382,  0.3238, -0.3968,  ...,  0.1442, -0.1680,  0.1014],\n",
       "         [ 0.4283,  0.3252, -0.3942,  ...,  0.1360, -0.1700,  0.0911],\n",
       "         [ 0.4351,  0.3369, -0.3934,  ...,  0.1294, -0.1701,  0.0882],\n",
       "         ...,\n",
       "         [ 0.4276,  0.3310, -0.3972,  ...,  0.1280, -0.1779,  0.0936],\n",
       "         [ 0.4363,  0.3313, -0.3875,  ...,  0.1383, -0.1615,  0.0775],\n",
       "         [ 0.4270,  0.3180, -0.3940,  ...,  0.1438, -0.1715,  0.0945]],\n",
       "\n",
       "        [[ 0.3994,  0.2882, -0.3421,  ...,  0.1332, -0.1352,  0.0814],\n",
       "         [ 0.3903,  0.2972, -0.3413,  ...,  0.1072, -0.1387,  0.0663],\n",
       "         [ 0.3986,  0.2787, -0.3376,  ...,  0.1395, -0.1232,  0.0556],\n",
       "         ...,\n",
       "         [ 0.3947,  0.2832, -0.3396,  ...,  0.1345, -0.1363,  0.0783],\n",
       "         [ 0.3929,  0.2860, -0.3413,  ...,  0.1318, -0.1355,  0.0767],\n",
       "         [ 0.3941,  0.2754, -0.3402,  ...,  0.1454, -0.1323,  0.0783]],\n",
       "\n",
       "        [[ 0.4526,  0.3301, -0.4109,  ...,  0.1562, -0.1767,  0.0740],\n",
       "         [ 0.4395,  0.3222, -0.4064,  ...,  0.1545, -0.1802,  0.0672],\n",
       "         [ 0.4429,  0.3367, -0.4050,  ...,  0.1414, -0.1800,  0.0595],\n",
       "         ...,\n",
       "         [ 0.4487,  0.3201, -0.4108,  ...,  0.1673, -0.1741,  0.0699],\n",
       "         [ 0.4467,  0.3277, -0.4084,  ...,  0.1570, -0.1761,  0.0685],\n",
       "         [ 0.4444,  0.3221, -0.4102,  ...,  0.1615, -0.1760,  0.0702]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSA = SelfAttention()\n",
    "LN = nn.LayerNorm(embeddings.shape, bias=False)\n",
    "\n",
    "MSA(LN(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, bias = False, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(embed_dim, embed_dim * 4, bias=bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(embed_dim * 4, embed_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim = 768, bias = False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim, bias=bias)\n",
    "        self.attn = SelfAttention(embed_dim, bias=bias)\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim, bias=bias)\n",
    "        self.mlp = MLP(embed_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim = 768, num_layers = 4, out_dim = 10, bias = False, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            pe = PatchEmbedding(),\n",
    "            drop = nn.Dropout(dropout),\n",
    "            h = nn.ModuleList([Block() for _ in range(num_layers)]),\n",
    "            ln_f = nn.LayerNorm(embed_dim)\n",
    "        ))\n",
    "        self.head = nn.Linear(embed_dim, out_dim, bias=False)\n",
    "\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.transformer.pe(x)\n",
    "        x = self.transformer.drop(emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        class_token = x[:, 0]\n",
    "        logits = self.head(class_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 28.42M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1456, -0.2529, -0.3532, -1.1192,  1.4267,  1.2951, -0.9670,  1.7261,\n",
       "          0.2198,  0.4939],\n",
       "        [-0.2264, -0.2649, -0.6843, -1.2652,  1.1855,  1.7658, -1.0856,  2.0121,\n",
       "          0.0788,  0.5883],\n",
       "        [-0.2941, -0.2332, -0.5577, -1.1553,  1.3977,  1.4724, -1.0573,  1.9429,\n",
       "         -0.0756,  0.6941],\n",
       "        [-0.2021, -0.2922, -0.6660, -1.2247,  1.4233,  1.6697, -1.2059,  1.9832,\n",
       "          0.0576,  0.7225],\n",
       "        [-0.4389, -0.3675, -0.6694, -1.2252,  1.2807,  1.2867, -1.2640,  1.8317,\n",
       "          0.0752,  0.5037],\n",
       "        [-0.2618, -0.4284, -0.4262, -1.3106,  1.2286,  1.4338, -1.0285,  1.9554,\n",
       "          0.2316,  0.5074],\n",
       "        [-0.1456, -0.4101, -0.7117, -1.2143,  1.3792,  1.4249, -1.0340,  1.6290,\n",
       "          0.3441,  0.5831],\n",
       "        [-0.4597, -0.2289, -0.4271, -1.0971,  1.1724,  1.6888, -1.1043,  1.7292,\n",
       "          0.0644,  0.4074],\n",
       "        [-0.2236, -0.3282, -0.6501, -1.2261,  1.4172,  1.5638, -1.2377,  1.8906,\n",
       "          0.2984,  0.5549],\n",
       "        [-0.3231, -0.0838, -0.5170, -1.1416,  1.4326,  1.3826, -0.8676,  2.0010,\n",
       "          0.0615,  0.6044]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT()\n",
    "vit(torch.stack([train_data[i][0] for i in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
